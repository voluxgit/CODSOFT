# -*- coding: utf-8 -*-
"""customer_churn_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JvSVBe6P-OQBBTzdtgmKtVDjJ2h9mWVd

## CODSOFT INTERNSHIP
varad patil

### Adding the dataset from kaggle
"""

!pip install kaggle

from google.colab import drive
drive.mount('/content/drive')

"""### make a temporary directory"""

import os
os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive/Colab Notebooks/kaggle_dataset'

!pwd

# Commented out IPython magic to ensure Python compatibility.
# %cd drive/MyDrive/Colab Notebooks/kaggle_dataset

!pwd

!kaggle datasets download -d shantanudhakadd/bank-customer-churn-prediction

!unzip bank-customer-churn-prediction.zip

"""### Importing Library"""

from logging import warning
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('Churn_Modelling.csv')

df.head()

df.columns

print('df:', df.shape)

df.duplicated().sum()

df.isnull().sum()

df.isnull().sum()

df.info()

df.describe()

plt.figure(figsize=(20,10))
sns.heatmap(df.corr(), annot = True)

col = []
for i in df.columns:
  num = len(df[i].unique())
  print(i,':', str(num) + str(' Distinct values'))
  if num < 10:
    col.append(i)

for i in col:
  print(df[i].value_counts(), '\n')

"""### Data Visulaization"""

for i in col:
  sns.countplot(x=i, data=df)
  plt.title(i)
  plt.show()

plt.hist(df['Age'], edgecolor='black')
plt.title('Age Distribution')
plt.show()

sns.set(style="whitegrid")
j = col.pop()
for i in col:
  sns.countplot(x=i, hue=j, data=df)
  plt.title( i + ' vs ' + j)
  plt.show()

"""### Data preprocessing

selecting the columns
"""

df.head()

df.drop(columns=['RowNumber', 'CustomerId', 'Surname'], inplace=True )

df.head()

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['Gender'] = le.fit_transform(df['Gender'])

df = pd.get_dummies(data = df, columns=['Geography'], drop_first=True)

df.head()

a = df.iloc[:,-3:-2]

a.head()

df.drop(columns=['Exited'], inplace = True)

df.head()

df = pd.concat([df, a], axis=1)

df.head()

plt.figure(figsize=(20,10))
sns.heatmap(df.corr(), annot = True)

x = df.iloc[:, :-1].values
y = df.iloc[:,-1].values

"""Splitting the dataset"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)

print(x_train.shape, x_test.shape)
print(y_train.shape, y_test.shape)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

"""Training and testing the models"""

from sklearn.linear_model import LogisticRegression
log = LogisticRegression(random_state = 42)
log.fit(x_train, y_train)

from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
R = RandomForestClassifier(n_estimators=10 ,random_state = 42)
R.fit(x_train, y_train)

clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, random_state=42)
clf.fit(x_train, y_train)

classifier = [log, R, clf]
model = ['Logistic Regression', 'Random Forest Classifier', 'Gradient Boosting Classifier']

"""## Making the Confusion Matrix"""

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay
for i in range(len(classifier)):
  y_pred = classifier[i].predict(x_test)
  cm = confusion_matrix(y_test, y_pred)
  accuracy = accuracy_score(y_test, y_pred)*100
  print('\nfor ' + str(model[i]) + ':\n')
  disp = ConfusionMatrixDisplay(confusion_matrix=cm)
  plt.rcParams['axes.grid'] = False
  disp.plot()
  print(accuracy)
  print(classification_report(y_test, y_pred))
  plt.show()

"""## Applying k-Fold Cross Validation"""

from sklearn.model_selection import cross_val_score
for i in classifier:
  accuracies  = cross_val_score(estimator=i, X = x_train, y = y_train, cv = 10)
  print('Accuracy: {:.2f} %'.format(accuracies.mean()*100))
  print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))

"""Hyper parameter tuning

## Applying Grid Search to find the best model and the best parameters for gradient boosting
"""

from sklearn.model_selection import GridSearchCV
parameters = [{'n_estimators': [*range(10,200,30)], 'learning_rate':np.arange(0, 1, 0.25), 'max_depth':[*range(1, 5, 2)]}]
grid_search = GridSearchCV(estimator = clf,
                           param_grid = parameters,
                           scoring = 'accuracy',
                           cv = 10,
                           n_jobs = -1)
grid_search.fit(x_train, y_train)
best_accuracy = grid_search.best_score_
best_parameters = grid_search.best_params_
print("Best Accuracy: {:.2f} %".format(best_accuracy*100))
print("Best Parameters:", best_parameters)

clf = GradientBoostingClassifier(n_estimators=10, learning_rate=0.5, random_state=42, max_depth = 3)
clf.fit(x_train, y_train)

"""## Applying Grid Search to find the best model and the best parameters for Random forest"""

parameters = [{'n_estimators': [*range(10,200,30)], 'max_depth':[*range(1, 20, 2)]}]
grid_search = GridSearchCV(estimator = R,
                           param_grid = parameters,
                           scoring = 'accuracy',
                           cv = 10,
                           n_jobs = -1)
grid_search.fit(x_train, y_train)
best_accuracy = grid_search.best_score_
best_parameters = grid_search.best_params_
print("Best Accuracy: {:.2f} %".format(best_accuracy*100))
print("Best Parameters:", best_parameters)

R = RandomForestClassifier(n_estimators=160 ,random_state = 42, max_depth = 9)
R.fit(x_train, y_train)

"""## Applying Grid Search to find the best model and the best parameters for Logistic regression"""

parameters = [{'C': [0.001, 0.01, 0.1, 1, 10, 100],
    'penalty': ['l1', 'l2', 'elasticnet'],
    'solver' :['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'aga']}]
grid_search = GridSearchCV(estimator = log,
                           param_grid = parameters,
                           scoring = 'accuracy',
                           cv = 10,
                           n_jobs = -1)
grid_search.fit(x_train, y_train)
best_accuracy = grid_search.best_score_
best_parameters = grid_search.best_params_
print("Best Accuracy: {:.2f} %".format(best_accuracy*100))
print("Best Parameters:", best_parameters)

log = LogisticRegression(C= 0.01, penalty = 'l2' ,random_state = 42)
log.fit(x_train, y_train)

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay
for i in range(len(classifier)):
  y_pred = classifier[i].predict(x_test)
  cm = confusion_matrix(y_test, y_pred)
  accuracy = accuracy_score(y_test, y_pred)*100
  print('\nfor ' + str(model[i]) + ':\n')
  disp = ConfusionMatrixDisplay(confusion_matrix=cm)
  plt.rcParams['axes.grid'] = False
  disp.plot()
  print(accuracy)
  print(classification_report(y_test, y_pred))
  plt.show()

from sklearn.model_selection import cross_val_score
for i in classifier:
  accuracies  = cross_val_score(estimator=i, X = x_train, y = y_train, cv = 10)
  print('Accuracy: {:.2f} %'.format(accuracies.mean()*100))
  print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))