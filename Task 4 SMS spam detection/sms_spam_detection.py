# -*- coding: utf-8 -*-
"""SMS_SPAM_DETECTION.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RT3sUmG7jizen3lBHtMhW0hk9eVdab07

## CODSOFT INTERNSHIP
varad patil

### Adding the dataset from kaggle
"""

!pip install kaggle

from google.colab import drive
drive.mount('/content/drive')

"""### make a temporary directory"""

import os
os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive/Colab Notebooks/kaggle_dataset'

!pwd

# Commented out IPython magic to ensure Python compatibility.
# %cd drive/MyDrive/Colab Notebooks/kaggle_dataset

!pwd

!kaggle datasets download -d uciml/sms-spam-collection-dataset

!unzip sms-spam-collection-dataset.zip

"""### Importing Library"""

from logging import warning
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('spam.csv', encoding='latin-1')

df.head()

df.dropna(axis=1, inplace=True)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['v1'] = le.fit_transform(df['v1'])

df.head()

print('df:', df.shape)

df.rename(columns={"v1":"Category","v2":"Message"}, inplace = True)

df['num_characters'] = df['Message'].apply(len)

from nltk import corpus
import re
import nltk
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

df['num_words'] = df['Message'].apply(lambda x:len(nltk.word_tokenize(x)))

df['num_sentences'] = df['Message'].apply(lambda x:len(nltk.sent_tokenize(x)))

df.head()

df.duplicated().sum()

df.drop_duplicates(keep='first', inplace=True)
df.reset_index(drop = True, inplace = True)

df.head(105)

df.duplicated().sum()

df.isna().sum()

df.isnull().sum()

df.info()

df.describe()

sns.heatmap(df.corr(),annot=True)
plt.xticks(rotation=90)
plt.show()

"""### Cleaning the data"""

all_stopwords = stopwords.words('english')
print(all_stopwords)



corpus = []
for i in range(0, df.shape[0]):
  review = re.sub('[^a-zA-z]', ' ', df['Message'][i])
  review = review.lower()
  #print(review)
  review = review.split()
  #print(review)
  ps = PorterStemmer()
  review = [ps.stem(word) for word in review if not word in all_stopwords]
  #print(review)
  review = ' '.join(review)
  #print(review)
  corpus.append(review)

corpus[0:5]

df['transformed'] = corpus

df.head()

"""### Data Visulaization"""

from wordcloud import WordCloud
wc = WordCloud(width=500,height=500,min_font_size=10,background_color='white')

spam_wc = wc.generate(df[df['Category'] == 1]['transformed'].str.cat(sep= ' '))

plt.figure(figsize=(15,6))
plt.title("spam email")
plt.imshow(spam_wc)

ham_wc = wc.generate(df[df['Category'] == 0]['transformed'].str.cat(sep= ' '))
plt.figure(figsize=(15,6))
plt.title("ham email")
plt.imshow(ham_wc)

from collections import Counter

spam_words = " ".join(df[df['Category'] == 1]['Message']).split()
ham_words = " ".join(df[df['Category'] == 0]['Message']).split()

spam_word_freq = Counter([word.lower() for word in spam_words if word.lower() not in all_stopwords and word.isalpha()])

plt.figure(figsize=(10, 6))
plt.bar(*zip(*spam_word_freq.most_common(10)), color='r')
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.title('Top 10 Most Common Words in Spam Emails')
plt.xticks(rotation=45)
plt.show()

ham_word_freq = Counter([word.lower() for word in ham_words if word.lower() not in all_stopwords and word.isalpha()])

plt.figure(figsize=(10, 6))
plt.bar(*zip(*ham_word_freq.most_common(10)), color='g')
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.title('Top 10 Most Common Words in Ham Emails')
plt.xticks(rotation=45)
plt.show()

sns.countplot(x='Category', data=df)
plt.title('Category')
plt.show()

"""### Data preprocessing"""

from sklearn.feature_extraction.text import TfidfVectorizer
tf = TfidfVectorizer()
x = tf.fit_transform(corpus).toarray()
y = df.iloc[:, 0:1].values

print('x shape', x.shape)
print('y shape', y.shape)

"""### Splitting the dataset"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)

print(x_train.shape, x_test.shape)
print(y_train.shape, y_test.shape)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

"""Training and testing the models"""

from sklearn.linear_model import LogisticRegression
log = LogisticRegression(random_state = 42)
log.fit(x_train, y_train)

from sklearn.naive_bayes import GaussianNB, BernoulliNB
GB = GaussianNB()

GB.fit(x_train, y_train)

CB = BernoulliNB()
CB.fit(x_train, y_train)

from sklearn.svm import SVC
svc = SVC(kernel = 'linear', random_state = 0)
svc.fit(x_train, y_train)

classifier = [log, svc, GB, CB]
model = ['Logistic', 'Support Vector', 'Naive Bayes GB', 'Naive Bayes CB']

"""## Making the Confusion Matrix"""

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay
for i in range(0, len(classifier)):
  y_pred = classifier[i].predict(x_test)
  cm = confusion_matrix(y_test, y_pred)
  accuracy = accuracy_score(y_test, y_pred)*100
  print('\nfor ' + str(model[i]) + ':\n')
  disp = ConfusionMatrixDisplay(confusion_matrix=cm)
  plt.rcParams['axes.grid'] = False
  disp.plot()
  print('Accuracy: ',accuracy)
  print(classification_report(y_test, y_pred))
  plt.show()

"""## Applying k-Fold Cross Validation"""

from sklearn.model_selection import cross_val_score
for i in classifier:
  accuracies  = cross_val_score(estimator=i, X = x_train, y = y_train, cv = 10)
  print('Accuracy: {:.2f} %'.format(accuracies.mean()*100))
  print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))